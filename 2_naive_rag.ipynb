{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "6aix3j2ibphro23nt2bn",
   "authorId": "485345030254",
   "authorName": "MLP_USER_1",
   "authorEmail": "",
   "sessionId": "7a18f7cb-e1f6-40c4-af64-86f033597acc",
   "lastEditTime": 1765838370532
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6550d0b7-3486-4f5a-8f06-0451afe8998e",
   "metadata": {
    "collapsed": false,
    "name": "naive_rag",
    "resultHeight": 183
   },
   "source": [
    "# Část 2\n",
    "Nyní zkusíme využít oklasifikované události z předchozí části a upravit jimi znalostní bázi našeho LLM modelu.\n",
    "\n",
    "Nezapomeňte si opět nainstalovat balíček _snowflake-ml-python_ z tlačítka _Packages_ v horní liště."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "from snowflake.cortex import complete, CompleteOptions\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b547aa14-2efd-4e39-b1b5-f2e5b96d35f2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "helper_functions",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Pomocne fce\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Pomocna funkce na prevedeni radku do JSON formatu\n",
    "def jsonify_row(row):\n",
    "    return json.dumps(row.as_dict(), indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# Ocisteni uzivatelskeho jmena\n",
    "def user_suffix(session):\n",
    "    if name := session.get_current_user():\n",
    "        return name[1:-1]\n",
    "    # nemelo by nastat, ale at mame string\n",
    "    return f\"NONE_{random.randint(1,1000)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6107d-b7c0-4db1-ba49-f8b4276adeac",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "model = \"llama3.1-70b\"\n",
    "user_tables_suffix = user_suffix(session)\n",
    "categorization_table = f\"NI_MLP_LAB.{user_tables_suffix}.EVENT_CATEGORIZATION_{user_tables_suffix}\"\n",
    "\n",
    "df_categorization = session.read.table(categorization_table)\n",
    "df_test_questions = session.read.table(\"NI_MLP_LAB.LAB.QA\")\n",
    "test_q_collected = df_test_questions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9dea1c-a401-4b8b-b0c6-5fddf44dd0ae",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "resultHeight": 422
   },
   "outputs": [],
   "source": [
    "st.dataframe(df_test_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe07d6-1054-44ed-87a8-096e9741be1d",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "def get_random_test_question():\n",
    "    idx = random.randint(0, len(test_q_collected) - 1)\n",
    "    return test_q_collected[idx]\n",
    "\n",
    "\n",
    "def get_question(idx):\n",
    "    return test_q_collected[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ac041-eb54-4641-a681-ffedcd463c8d",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "resultHeight": 104
   },
   "outputs": [],
   "source": "# Zkuste si, jake odpovedi nam dava model na otazky z let 2024-25\n# Experimentujte take s parametrem temperature\n\nquestion = get_random_test_question()  # Nebo si ulozte jednu predem a upravujte temperature\nquestion = get_question(1)\n\nsystem_prompt = \"You are an assistant specialized in history. Provide short and accurate answers to user's questions.\"\nuser_prompt = question.QUESTION\nexpected_answer = question.ANSWER\n\noptions = CompleteOptions(temperature=0.2)\n\nclassification = complete(\n    model,\n    [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_prompt},\n    ],\n    options=options\n)\n\nprint(f\"\"\"Question to model: {user_prompt}\nExpected answer: {expected_answer}\nModel answer: {classification}\n\"\"\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e7c464-2c3f-41ca-8c1e-e6382505f5df",
   "metadata": {
    "language": "sql",
    "name": "cell6",
    "resultHeight": 111
   },
   "outputs": [],
   "source": [
    "-- Mimochodem ve Snowflake noteboocich je mozne kombinovat Python a SQL\n",
    "-- Volani Complete fce v SQL muze vypadat treba takto\n",
    "SELECT SNOWFLAKE.CORTEX.COMPLETE(\n",
    "    'llama3.1-8b',\n",
    "    ARRAY_CONSTRUCT(\n",
    "        OBJECT_CONSTRUCT(\n",
    "            'role', 'system', \n",
    "            'content', 'You are a helpful assisstant trying to answer user''s questions'),\n",
    "        OBJECT_CONSTRUCT(\n",
    "            'role', 'user', \n",
    "            'content', 'When is your trainig data cut-off?')\n",
    "    ),\n",
    "    OBJECT_CONSTRUCT(\n",
    "        'temperature', 0.1\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625899e-5984-47ea-868f-250c32b53f3e",
   "metadata": {
    "collapsed": false,
    "name": "cell8",
    "resultHeight": 169
   },
   "source": [
    "## Úloha - naivní RAG\n",
    "A nyní zkusíme využít námi oklasifikované události z roku 2024, abychom rozšířili znalosti námi používaného LLM modelu.\n",
    "\n",
    "V první fázi zkusíme, jestli dostaneme očekávané výsledky na události z roku 2024 ve sportovní kategorii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f4b78-7da5-4610-8d30-35d87a620b95",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "only_sport_events = df_categorization.filter(col(\"CATEGORY_ID\") == 2)\n",
    "only_sport_q = (\n",
    "    df_test_questions\n",
    "    .join(\n",
    "        only_sport_events,\n",
    "        [\"EVENT_ID\"]\n",
    "    )\n",
    "    .filter(col(\"CATEGORY_ID\") == 2)\n",
    "    .select(\"QUESTION\", \"ANSWER\")\n",
    "    .limit(5)  # You may remove / raise later\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55901d3e-6e41-4f1b-82d7-05c294bba13a",
   "metadata": {
    "language": "python",
    "name": "cell11",
    "resultHeight": 216
   },
   "outputs": [],
   "source": [
    "only_sport_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fef5b8-efc3-46b0-9c37-d329ab3d1f17",
   "metadata": {
    "language": "python",
    "name": "cell13",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "info = \"/n\".join(jsonify_row(r) for r in only_sport_events.select(\"EVENT_TEXT\").limit(3).collect())  # Raise limit when you are sure your prompt works\n\nsystem_prompt = f\"\"\"You are a helpful assistant. Use short answers, at most 5 words long. When answering user's question, use following information (each information is a JSON object with element EVENT_TEXT describing the event):\n{info}\n\"\"\"\n\n# Vyzkousejte, jak se budou lisit odpovedi pro ruzne hodnoty temperature\noptions = CompleteOptions(temperature=0.1)\n\nsports_list = []\nfor row in only_sport_q.collect():\n    answer = complete(\n        model,\n        [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": row.QUESTION},\n        ],\n        options=options\n    )\n    sports_list.append(\n        [row.QUESTION, row.ANSWER, answer]\n    )\n\nsport_df = session.create_dataframe(sports_list, [\"question\", \"expected\", \"llm_answer\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec193043-d9ec-476a-9a5d-e977001cb9ab",
   "metadata": {
    "language": "python",
    "name": "cell17",
    "resultHeight": 340
   },
   "outputs": [],
   "source": [
    "st.dataframe(sport_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c53b8b-8276-421a-bfc7-b3e5def10585",
   "metadata": {
    "collapsed": false,
    "name": "cell9",
    "resultHeight": 252
   },
   "source": [
    "Nyní se pokuste seskládat všechny výše použité kódy k tomu, abyste sestavili _vyhodnocovacího agenta_. Ten bude mít podobu funkce (viz níže), která na vstupu dostane uživatelský prompt. Našim cílem bude kategorizovat uživatelovu otázku, na základě toho vybrat události pouze z relevantní kategorie a následně odpovědět.\n",
    "\n",
    "Pokud uživatel zadá otázku mimo naše 4 kategorie, buď mu řekněte, že na takové otázky neodpovídáte, nebo nechcte odpovědět LLM bez poskytnutí znalostní báze.\n",
    "\n",
    "\\* Pozn.: Kontextové okno llamy je dost velké, aby se do něj vešly všechny naše kategorizované události. Toto řešení ale nechceme ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41845f99-f9df-4f1d-8823-7a6ffb02c781",
   "metadata": {
    "language": "python",
    "name": "cell12"
   },
   "outputs": [],
   "source": [
    "def perform_naive_rag(user_prompt: str) -> str:\n",
    "    # Faze 1 - kategorizujme uzivatelovu otazku\n",
    "    system_prompt_1 = \"Classify user prompt...\"\n",
    "    category = complete(...)  # mozna bude nutne jeste nejak transformovat\n",
    "\n",
    "    # Vybereme kategorii znalostni baze dle klasifikace user promptu\n",
    "    relevant_documents = (\n",
    "        df_categorization\n",
    "        .filter(col(\"CATEGORY\") == category)\n",
    "    )\n",
    "\n",
    "    system_prompt_2 = \"...\"   \n",
    "    # Odpovime na uzivatelovu otazku\n",
    "    answer = complete(...)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c154ac75-291f-4829-8f48-08b07d7d87d1",
   "metadata": {
    "language": "python",
    "name": "cell14",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Vyzkousime si odpovedi ...\n",
    "test_questions = random.sample(df_test_questions.collect(), k=10)\n",
    "\n",
    "naive_rag_ans = []\n",
    "for tq in test_questions:\n",
    "    llm_answer = perform_naive_rag(tq.QUESTION)\n",
    "    naive_rag_ans.append([tq.QUESTION, tq.ANSWER, llm_answer])\n",
    "\n",
    "naive_rag_df = session.create_dataframe(naive_rag_ans, [\"question\", \"expected\", \"llm_answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8511417-1c3f-4d76-8eff-485eedd643fe",
   "metadata": {
    "collapsed": false,
    "name": "cell15",
    "resultHeight": 227
   },
   "source": [
    "V reálné praxi samozřejmě máme znalostní bázi podstatně větší a potřebujeme tedy používat efektivnější metody přístupu k ní. Typicky každý dokument, který chceme zařadit do znalostní báze, rozdělíme na menší části, nad kterými následně spočítáme _embedding_ (transformujeme text do číselného vektoru).\n",
    "\n",
    "Dokumenty včetně embeddingů následně uložíme do _vektorové databáze_, od které očekáváme, že dokáže efektivně vyhledat _n_ nejbližších dokumentů na základě podobnosti _embeddingů_. \n",
    "\n",
    "V další fázi si tedy zkusíme spočítat embeddingy nad našimi událostmi a vyhledávat mezi nimi na základě našich dotazů.\n",
    "\n",
    "Pro úspěšné spuštění další buňky je potřeba do našeho prostředí doinstalovat balíček _langchain-text-splitters_ (obdobně jako jsme instalovali _snowflake-ml-python_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f3abb-fcef-4af3-bdfd-ef45735108f7",
   "metadata": {
    "language": "python",
    "name": "cell18",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Snowflake embedding modely maji limit 512 tokenu, takze je potreba nase texty rozdelit\n",
    "# Muzete zkusit experimentovat s parametry nebo jinymi splittery\n",
    "# (Inicialni parametry pro chunk size/overlap jsou extremne nizke, abychom rozdelili i nase pomerne kratke texty)\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=25,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "all_event_texts = df_categorization.select(\"EVENT_TEXT\").collect()\n",
    "texts = text_splitter.create_documents([r.EVENT_TEXT for r in all_event_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416eee9a-6ee6-4252-913f-f653f0ccc194",
   "metadata": {
    "language": "python",
    "name": "cell16",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "from snowflake.cortex import embed_text_768, embed_text_1024\n",
    "\n",
    "\n",
    "# Zaexperimentujte si s ruznymi modely a delkou vektoru \n",
    "embedding_model = \"snowflake-arctic-embed-m\"  \n",
    "# embedding_model = \"e5-base-v2\"\n",
    "\n",
    "vector_table = f\"VECTOR_{user_tables_suffix}\"\n",
    "\n",
    "chunks_df = session.create_dataframe([t.page_content for t in texts], [\"text\"])\n",
    "chunks_embedded = chunks_df.withColumn(\n",
    "    \"embedding\", \n",
    "    embed_text_768(embedding_model, chunks_df[\"TEXT\"])\n",
    ")\n",
    "chunks_embedded.write.save_as_table(vector_table, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab43152-804f-41ef-804f-b41caa0686a9",
   "metadata": {
    "language": "python",
    "name": "cell26",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Vyhledavani podle kosinove vzdalenosti bohuzel jeste neni v Python API, takze si musime \n",
    "# pomoct takto:\n",
    "user_query = \"What happened in Czech politics and sports in 2025?\"\n",
    "emb_limit = 3\n",
    "df = session.sql(f\"\"\"\n",
    "    SELECT\n",
    "        t.text,\n",
    "        VECTOR_COSINE_SIMILARITY(\n",
    "            t.embedding,\n",
    "            SNOWFLAKE.CORTEX.EMBED_TEXT_768(\n",
    "                ?, \n",
    "                ?\n",
    "            )\n",
    "        ) AS similarity\n",
    "    FROM {vector_table} AS t\n",
    "    ORDER BY similarity DESC\n",
    "    LIMIT {emb_limit}\n",
    "    \"\"\",\n",
    "    params=[embedding_model, user_query]            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e97d10-2b57-4b3f-85a8-f0a02ce7f1b5",
   "metadata": {
    "language": "python",
    "name": "cell24",
    "resultHeight": 181
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c1e7aa-45a7-45ed-acf9-ad753df75bd3",
   "metadata": {
    "collapsed": false,
    "name": "cell20",
    "resultHeight": 108
   },
   "source": [
    "Nyní vytvořte nového agenta analogického k _perform_naive_rag_, který ovšem pro vyhledávání relevantních dokumentů budou používat před chvílí vytvořenou tabulku s embeddingy jednotlivých událostí. \n",
    "\n",
    "Experimentuje s různými modely, velikostmi embeddingů a počtem dokumentů, které do modelu dodáte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1906c8d-89a4-478d-b3f1-7987c6f33e01",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "def embedding_rag(user_prompt):\n",
    "    pass  # Now it's your task ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289391cf-6c93-44d1-9d26-26da62bac31b",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "# Vyzkousime si odpovedi ...\n",
    "rag_test_questions = random.sample(df_test_questions.collect(), k=10)\n",
    "\n",
    "embedding_rag = []\n",
    "for tq in rag_test_questions:\n",
    "    llm_answer = perform_naive_rag(tq.QUESTION)\n",
    "    embedding_rag.append([tq.QUESTION, tq.ANSWER, llm_answer])\n",
    "\n",
    "embedding_rag_df = session.create_dataframe(embedding_rag, [\"question\", \"expected\", \"llm_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aad528-3253-415f-8b81-43a3e7e8c93b",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": [
    "embedding_rag_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d43d9a-e023-4600-83cb-cba1d751febe",
   "metadata": {
    "collapsed": false,
    "name": "cell10",
    "resultHeight": 428
   },
   "source": [
    "## Bonus\n",
    "Vytvořte LLM, který _lže_. Vytvořte si vlastní znalostní bázi, která bude obsahovat nepravdivé informace, a pokuste se s jejím využitím nastavit systémový prompt tak, aby model bez zaváhání odpovídal nepravdivě na všeobecně známá historická fakta (přesné otázky si vymyslete sami, nebo si témata nechte vygenerovat ;), ale jako inspiraci dáváme):\n",
    "\n",
    "- Samostatná Česká republika nevznikla 1.1.1993\n",
    "- Prvním člověkem ve vesmíru byl Jára Cimrman\n",
    "- Americkou občanskou válku vyhrála Konfederace\n",
    "- Richard Nixon vyhrál prezidentské volby už v roce 1960 a J.F. Kennedy tak zůstal až do své smrti roku 2012 řadovým senátorem za Massachusetts\n",
    "- Otcem Luka Skywalkera je Obi-Wan Kenobi\n",
    "\n",
    "Zajistěte, aby LLM podávalo odpovědi jako fakta, kterým opravdu věří. Především zamezte tomu, aby odpovídalo stylem \"Podle mé znalostní báze se to sice stalo takhle, ale to je špatně a ve skutečnosti ...\"."
   ]
  }
 ]
}
